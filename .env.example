# Copy this to .env file

# LLM Backend
MODEL_BACKEND=ollama
MODEL_NAME=llama3.1:8b-instruct-q5_K_M
MODEL_TEMPERATURE=0.7
MODEL_MAX_TOKENS=2048

# Ollama Config
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_NUM_THREAD=16
OLLAMA_NUM_BATCH=512

# MLX Config (Apple Silicon)
MLX_MODEL_PATH=mlx_models/llama3-8b
MLX_MAX_TOKENS=512

# Execution
PARALLEL_MODE=true
MAX_CONCURRENT_TASKS=4

# Memory
MEMORY_TYPE=chroma

# Claude Desktop
CLAUDE_DELAY=12.0
CLAUDE_PASTE_DELAY=0.3
CLAUDE_COPY_DELAY=0.3

# Metrics
ENABLE_METRICS=true
LOG_LEVEL=INFO

# API Keys (optional)
OPENAI_API_KEY=
ANTHROPIC_API_KEY=

# HuggingFace (embeddings + optional inference)
HF_TOKEN=                                    # Get from huggingface.co/settings/tokens
HF_EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
HF_MODEL=meta-llama/Llama-3.1-8B-Instruct   # For inference (optional)

# HuggingFace Pro Backend (optional)
# MODEL_BACKEND=huggingface
# HF_MODEL=meta-llama/Llama-3.1-8B-Instruct
# HF_MAX_TOKENS=1024
# HF_COST_LIMIT_USD=10.00
