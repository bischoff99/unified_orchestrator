# v2.1 "Hardening + UX" - Progress Log

## 2025-10-22 - Part 1: Core Infrastructure ✅ COMPLETED

### Summary
Implemented foundational v2.1 features: typed events, resume-from-failure, and deterministic caching.

### Changes Made

#### 1. Enhanced Events System (src/core/events.py)
- **Event TypedDict**: Defined strict schema with `ts`, `level`, `job_id`, `type`, `step`, `data`
- **ISO8601 Timestamps**: Auto-added with 'Z' suffix for UTC
- **Validation**: emit() now validates required fields (type, job_id)
- **New Event Methods**:
  - `llm_request(job_id, step_id, provider, model, prompt_tokens)`
  - `llm_response(job_id, step_id, provider, duration_s, tokens_in, tokens_out, success)`
  - `file_written(job_id, step_id, path, sha256, wrote, reason)`
  - `cache_hit(job_id, step_id, cache_key)`
  - `cache_miss(job_id, step_id, cache_key)`
- **Enhanced Filtering**: `filter_events()` now supports level filtering

#### 2. Resume-from-Failure (src/core/dag.py, src/core/manifest.py)
- **Manifest Fields**: Added `completed_steps` and `pending_steps` lists
- **read_completed_steps()**: Function to parse events.jsonl for step.succeeded events
- **run_dag() Enhancement**:
  - New `resume: bool = False` parameter
  - Reads completed steps from events.jsonl when resume=True
  - Skips completed steps, only executes pending work
  - Emits `step.skipped` events for resumed steps
- **RunManager**: Updated `update_manifest()` to accept completed/pending step lists

#### 3. Deterministic Cache System (src/core/cache.py)
- **compute_cache_key()**: SHA256 hash of:
  - Provider config (name, model, opts)
  - Step identifier
  - Input data (JSON-serialized)
  - Code version (git HEAD or file hash)
- **get_code_version()**: Auto-detects git commit or hashes key source files
- **Cache I/O**: `read_cache()`, `write_cache()`, `cache_path()` utilities
- **Storage**: `runs/<job_id>/.cache/<key>.json`

#### 4. Orchestrator Integration (src/orchestrator/dag_orchestrator.py)
- **_call_provider_with_cache()**: New method wrapping provider calls
  - Computes deterministic cache key
  - Checks cache before calling provider
  - Emits cache.hit or cache.miss events
  - Caches responses with metadata
- **Updated All Steps**: architect, builder, docs, qa now use caching
- **Provider Call Tracking**: Returns cache_hit status in step outputs

### Files Created
- `src/core/cache.py` (167 lines)
- `V2.1_IMPLEMENTATION_PLAN.md` (498 lines - task breakdown)
- `.cursor/V2.1_PROGRESS.md` (this file)

### Files Modified
- `src/core/events.py`: +154 lines (new event types and methods)
- `src/core/dag.py`: +62 lines (resume functionality)
- `src/core/manifest.py`: +10 lines (resume fields)
- `src/orchestrator/dag_orchestrator.py`: +96 net lines (cache integration)

### Git Commit
```
4c47654 feat(events): typed event schema with llm/file/cache events
```

### Testing Status
- ⏳ **Unit tests pending** (Part 3)
- ⏳ **Integration tests pending** (Part 3)
- ⚠️ **Manual smoke test recommended** before Part 2

### Key Benefits
1. **Observability**: All LLM calls, file writes, and cache operations are logged
2. **Resilience**: Jobs can resume from failure point without re-executing completed steps
3. **Efficiency**: Identical inputs skip LLM calls via deterministic cache
4. **Determinism**: Cache keys include code version for automatic invalidation

### Next Steps
- [ ] Part 2: Reliability (providers, circuit breaker, CLI) - 11 tasks
- [ ] Part 3: Testing + CI + Docs - 16 tasks

### API Examples

#### Resume a Failed Job
```python
# First run (fails at step 3)
results = await run_dag(dag, job_id, context, events, resume=False)

# Resume run (skips steps 1-2, starts at 3)
results = await run_dag(dag, job_id, context, events, resume=True)
# Emits: step.skipped for steps 1-2
```

#### Cache Hit Example
```python
# First run: cache.miss → provider.call → cache.write
response, hit = await orchestrator._call_provider_with_cache(
    "architect", messages, context, inputs
)
# hit = False, provider called

# Second run: cache.hit → return cached
response, hit = await orchestrator._call_provider_with_cache(
    "architect", messages, context, inputs  # Same inputs
)
# hit = True, provider NOT called
```

#### Event Stream Example
```json
{"ts": "2025-10-22T10:00:00Z", "level": "INFO", "job_id": "job_abc", "type": "job.started", "data": {...}}
{"ts": "2025-10-22T10:00:01Z", "level": "INFO", "job_id": "job_abc", "type": "step.started", "step": "architect"}
{"ts": "2025-10-22T10:00:02Z", "level": "INFO", "job_id": "job_abc", "type": "cache.miss", "step": "architect", "data": {"cache_key": "e3b0..."}}
{"ts": "2025-10-22T10:00:05Z", "level": "INFO", "job_id": "job_abc", "type": "llm.response", "step": "architect", "data": {"provider": "ollama", "duration_s": 3.2}}
{"ts": "2025-10-22T10:00:05Z", "level": "INFO", "job_id": "job_abc", "type": "step.succeeded", "step": "architect"}
```

---

**Part 1 Status**: ✅ COMPLETED (11/11 tasks)
**Overall Progress**: 11/38 tasks (28.9%)
**Est. Time Remaining**: 4-5 hours for Parts 2-3
