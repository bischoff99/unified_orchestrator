# Unified Orchestrator v2.1 Architecture

**Version:** 2.1.0  
**Last Updated:** 2025-10-22

## ğŸ“ System Overview

Unified Orchestrator is a Python-based multi-agent orchestration system that uses DAG (Directed Acyclic Graph) execution, LLM provider abstraction, and comprehensive observability to generate code projects efficiently.

### Core Design Principles

1. **Observability First**: Every operation emits structured events
2. **Resilience**: Resume from failure without re-execution
3. **Efficiency**: Deterministic caching eliminates redundant LLM calls
4. **Type Safety**: Pydantic v2 models throughout
5. **Async-Native**: Built on asyncio for concurrency

---

## ğŸ—ï¸ Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         CLI Interface                             â”‚
â”‚                    (orchestrator run/show/tail)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     DAG Orchestrator                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Job Spec  â”‚â”€â”€â”€â”€â–¶â”‚         DAG Builder                  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚  (architect â†’ {builder, docs} â†’ qa)  â”‚    â”‚
â”‚                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â–¼              â–¼              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Core Systems    â”‚ â”‚   Providers   â”‚ â”‚  FileStore   â”‚
â”‚                   â”‚ â”‚               â”‚ â”‚              â”‚
â”‚ â€¢ DAG Executor    â”‚ â”‚ â€¢ Ollama      â”‚ â”‚ â€¢ Safe Write â”‚
â”‚ â€¢ Event Emitter   â”‚ â”‚ â€¢ OpenAI      â”‚ â”‚ â€¢ SHA256     â”‚
â”‚ â€¢ Cache Manager   â”‚ â”‚ â€¢ Anthropic   â”‚ â”‚ â€¢ Locking    â”‚
â”‚ â€¢ Manifest Mgr    â”‚ â”‚ â€¢ MLX         â”‚ â”‚ â€¢ Events     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚              â”‚              â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Run Folder Structure                           â”‚
â”‚                                                                   â”‚
â”‚  runs/<job_id>/                                                   â”‚
â”‚    â”œâ”€â”€ manifest.json      (metadata + step tracking)             â”‚
â”‚    â”œâ”€â”€ events.jsonl       (all events, chronological)            â”‚
â”‚    â”œâ”€â”€ inputs/            (input files)                          â”‚
â”‚    â”œâ”€â”€ outputs/           (generated code)                       â”‚
â”‚    â”œâ”€â”€ logs/              (step logs)                            â”‚
â”‚    â”œâ”€â”€ artifacts/         (binary artifacts)                     â”‚
â”‚    â””â”€â”€ .cache/            (LLM response cache)                   â”‚
â”‚        â””â”€â”€ <sha256>.json  (cached responses)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”„ DAG Execution Model

### DAG Structure

The orchestrator uses a **Directed Acyclic Graph** to define workflow dependencies:

```python
# Example DAG: architect â†’ {builder, docs} â†’ qa

architect (no deps)
    â”œâ”€â†’ builder (depends on architect)
    â””â”€â†’ docs    (depends on architect)
        â””â”€â†’ qa (depends on builder + docs)
```

### Execution Phases

1. **Validation**: Check for cycles, verify dependencies exist
2. **Topological Sort**: Determine execution order
3. **Wave Execution**: Execute ready nodes in parallel
4. **Dependency Resolution**: Pass outputs from completed steps
5. **Result Collection**: Aggregate all step results

### Concurrency Control

```python
# Semaphore limits concurrent tasks
semaphore = asyncio.Semaphore(concurrency)

async with semaphore:
    # Execute step
    result = await execute_node(node)
```

**Benefits:**
- âœ… Parallel execution of independent steps
- âœ… Controlled resource usage
- âœ… Graceful failure handling

---

## ğŸ“Š Event System

### Event Schema (v2.1)

```typescript
Event {
  ts: string         // ISO8601 timestamp (e.g., "2025-10-22T10:00:00Z")
  level: "INFO" | "WARN" | "ERROR"
  job_id: string
  type: string       // Event type (see below)
  step?: string      // Optional step identifier
  data?: object      // Optional event-specific data
}
```

### Event Types

| Type | When Emitted | Data Fields |
|------|-------------|-------------|
| `job.started` | Job begins | project, provider, task |
| `job.succeeded` | Job completes | duration_s, artifacts |
| `job.failed` | Job fails | error, duration_s |
| `step.started` | Step begins | needs (dependencies) |
| `step.succeeded` | Step completes | duration_s, provider_calls |
| `step.failed` | Step fails | kind, message |
| `step.skipped` | Step skipped (resume) | reason |
| `llm.request` | Before LLM call | provider, model, prompt_tokens |
| `llm.response` | After LLM call | provider, duration_s, tokens, success |
| `file.written` | File written | path, sha256, wrote, reason |
| `cache.hit` | Cache hit | cache_key |
| `cache.miss` | Cache miss | cache_key |

### Event Storage

Events are stored in **ND-JSON** (Newline-Delimited JSON):

```bash
runs/job_abc123/events.jsonl
```

```json
{"ts":"2025-10-22T10:00:00Z","level":"INFO","job_id":"job_abc","type":"job.started",...}
{"ts":"2025-10-22T10:00:01Z","level":"INFO","job_id":"job_abc","type":"step.started","step":"architect"}
{"ts":"2025-10-22T10:00:05Z","level":"INFO","job_id":"job_abc","type":"llm.response","step":"architect",...}
```

**Benefits:**
- Streamable (tail -f)
- Parseable line-by-line
- Easy filtering with grep/jq

---

## ğŸ—„ï¸ Caching Strategy

### Deterministic Cache Keys

Cache keys are **SHA256 hashes** of:

```python
cache_input = {
  "provider": {
    "name": "ollama",
    "model": "llama3",
    "opts": {"temperature": 0.1}
  },
  "step": "architect",
  "inputs": {"task": "Build todo app"},
  "code_version": "abc123def"  # git commit or file hash
}

cache_key = SHA256(JSON.dumps(cache_input, sort_keys=True))
```

### Cache Lifecycle

```
1. Check cache: read_cache(cache_path)
   â”œâ”€ Hit:  Return cached response, emit cache.hit
   â””â”€ Miss: Call provider, emit cache.miss

2. Call Provider: provider.generate(messages)

3. Store Response: write_cache(cache_path, response)
```

### Automatic Invalidation

Cache keys include **code version**:

```python
# Get version
git_commit = subprocess.run(["git", "rev-parse", "HEAD"])
# OR
file_hash = SHA256(concat(source_files))

# Any code change â†’ new cache key â†’ cache miss
```

**Benefits:**
- âœ… No stale responses after code changes
- âœ… Safe to share caches across runs
- âœ… Explicit versioning for debugging

---

## ğŸ”„ Resume-from-Failure

### How It Works

1. **Job Fails Mid-Execution**
   - Steps 1-2 succeeded, step 3 failed
   - `events.jsonl` records: `step.succeeded` for 1-2

2. **User Runs with `--resume`**
   ```bash
   orchestrator run --spec job.yaml --resume
   ```

3. **Resume Logic**
   ```python
   # Read completed steps
   completed = read_completed_steps("runs/job_id/events.jsonl")
   # â†’ {"step1", "step2"}
   
   # Mark as complete in DAG
   for step_id in completed:
       results[step_id] = placeholder_result
       emit(step.skipped)
   
   # Execute only pending steps
   run_dag(resume=True)  # Skips step1, step2; runs step3+
   ```

### Resume Guarantees

- âœ… **Idempotent**: Re-running with `--resume` is safe
- âœ… **Correct Dependencies**: Skipped steps provide placeholder outputs
- âœ… **Observable**: `step.skipped` events show what was skipped

---

## ğŸ”Œ Provider Abstraction

### LLMProvider Protocol

```python
class LLMProvider(Protocol):
    name: str
    
    def generate(self, messages: list[dict], **opts) -> str:
        """Generate response from messages."""
        ...
```

### Provider Implementations

| Provider | Module | Key Features |
|----------|--------|--------------|
| **Ollama** | `src/providers/ollama.py` | Local models, performance tuning |
| **OpenAI** | `src/providers/openai.py` | GPT-4, streaming support |
| **Anthropic** | `src/providers/anthropic.py` | Claude, structured outputs |
| **MLX** | `src/providers/mlx.py` | Apple Silicon optimized |

### Provider Selection

```python
# config.py
PROVIDER = os.getenv("PROVIDER", "ollama")

def get_provider() -> LLMProvider:
    if PROVIDER == "ollama":
        return OllamaProvider()
    elif PROVIDER == "openai":
        return OpenAIProvider()
    # ...
```

---

## ğŸ’¾ FileStore with Events

### Safe Write Operations

```python
result = filestore.safe_write(
    path="main.py",
    content=code,
    mode="overwrite",
    emitter=events,
    job_id="job_123",
    step_id="builder"
)

# Returns:
{
    "path": Path("runs/job_123/outputs/main.py"),
    "sha256": "e3b0c44...",
    "size_bytes": 1234,
    "wrote": True,         # or False if content unchanged
    "reason": "created"    # or "nochange", "overwritten", "appended"
}

# Emits event:
{"type": "file.written", "step": "builder", "data": {...}}
```

### Write Modes

1. **create_new**: Fail if exists (unless content identical)
2. **overwrite**: Replace if content differs, skip if identical
3. **append**: Append content

---

## ğŸ“ Run Folder Structure

```
runs/
â””â”€â”€ job_abc123/
    â”œâ”€â”€ manifest.json          # Job metadata
    â”‚   â”œâ”€â”€ job_id, status
    â”‚   â”œâ”€â”€ started_at, finished_at
    â”‚   â”œâ”€â”€ steps (status, duration, artifacts)
    â”‚   â”œâ”€â”€ completed_steps    # For resume
    â”‚   â””â”€â”€ pending_steps
    â”‚
    â”œâ”€â”€ events.jsonl           # All events (ND-JSON)
    â”‚
    â”œâ”€â”€ inputs/                # Input files
    â”‚   â””â”€â”€ spec.yaml
    â”‚
    â”œâ”€â”€ outputs/               # Generated files
    â”‚   â””â”€â”€ project_name/
    â”‚       â”œâ”€â”€ main.py
    â”‚       â””â”€â”€ README.md
    â”‚
    â”œâ”€â”€ logs/                  # Step-specific logs
    â”‚   â”œâ”€â”€ architect.log
    â”‚   â”œâ”€â”€ builder.log
    â”‚   â””â”€â”€ qa.log
    â”‚
    â”œâ”€â”€ artifacts/             # Binary artifacts
    â”‚
    â””â”€â”€ .cache/                # LLM response cache
        â”œâ”€â”€ abc123def.json
        â””â”€â”€ 456789ghi.json
```

---

## ğŸ” Observability

### Querying Events

**Filter by type:**
```bash
grep '"type":"llm.response"' runs/job_123/events.jsonl | jq
```

**Calculate total LLM duration:**
```bash
jq -r 'select(.type=="llm.response") | .data.duration_s' events.jsonl | \
  awk '{sum+=$1} END {print sum}'
```

**Check cache effectiveness:**
```bash
echo "Hits: $(grep cache.hit events.jsonl | wc -l)"
echo "Misses: $(grep cache.miss events.jsonl | wc -l)"
```

---

## ğŸ¯ Performance Characteristics

### Parallel Execution

- **Baseline (v1.0)**: Sequential, ~60s for 4-step workflow
- **DAG (v2.0)**: Parallel, ~35s for same workflow
- **Cached (v2.1)**: 2nd run ~2s (95% speedup)

### Resource Usage

- **Memory**: ~200MB per job (depends on model)
- **Disk**: ~10MB per run (with cache)
- **Concurrency**: Default 4, configurable

---

## ğŸ”’ Safety & Reliability

### File Locking

```python
# Exclusive file locks prevent race conditions
with _file_lock(path):
    # Safe to write, no concurrent conflicts
    path.write_bytes(content)
```

### Idempotency

- **FileStore**: Writing identical content returns `wrote=False`
- **Resume**: Re-running with `--resume` skips completed work
- **Cache**: Same inputs always return same outputs

---

## ğŸ§ª Testing Architecture

### Test Pyramid

```
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   E2E (1)  â”‚  Golden tests
        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
        â”‚ Integrationâ”‚  DAG + Provider + FileStore
        â”‚    (10)    â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
        â”‚   Unit     â”‚  Individual functions
        â”‚   (100+)   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Test Suites

- `test_filestore.py`: Safe writes, idempotency, locking
- `test_resume.py`: Resume logic, skip behavior
- `test_cache.py`: Deterministic keys, cache hits
- `test_dag.py`: DAG validation, execution, concurrency
- `test_golden.py`: End-to-end output verification

---

## ğŸ“š Further Reading

- **MIGRATION.md**: Upgrading from v1.x to v2.x
- **CONTRIBUTING.md**: Development setup and guidelines
- **CHANGELOG.md**: Version history and changes
- **README.md**: Quick start and usage examples

---

**Architecture Version:** 2.1.0  
**Last Updated:** 2025-10-22
